{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ.setdefault(\"LOGURU_LEVEL\", \"INFO\" if not os.getenv(\"DEBUG\") else \"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from timeit import timeit\n",
    "\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from scipy import signal as libsignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv2d_vectorized(\n",
    "    signal: np.ndarray, kernel: np.ndarray, out_h: int, out_w: int\n",
    ") -> np.ndarray:\n",
    "    view_shape = (out_h, out_w, *kernel.shape)  # (\n",
    "    view_strides = (\n",
    "        signal.strides[0],  # jump to next row in signal,\n",
    "        signal.strides[1],  # jump to next col in signal,\n",
    "        signal.strides[0],  # jump to next row in window,\n",
    "        signal.strides[1],  # jump to next col in window,\n",
    "    )\n",
    "    views = as_strided(signal, view_shape, view_strides)  # (2,3,2,2)\n",
    "    reshaped_view = views.reshape(-1, kernel.size)\n",
    "    flattened_kernel = kernel.reshape(-1)\n",
    "    # [[0 0 1 2]\n",
    "    #  [0 0 2 2]\n",
    "    #  [0 0 2 1]\n",
    "    #  [1 2 0 0]\n",
    "    #  [2 2 0 0]\n",
    "    #  [2 1 0 0]]\n",
    "    return np.einsum(\"ji,i->j\", reshaped_view, flattened_kernel).reshape(out_h, out_w)\n",
    "\n",
    "\n",
    "def conv2d_naive(\n",
    "    signal: np.ndarray, kernel: np.ndarray, out_h: int, out_w: int\n",
    ") -> np.ndarray:\n",
    "    kernel_h, kernel_w = kernel.shape\n",
    "    conv = np.zeros((out_h, out_w))\n",
    "    for i in range(out_h):\n",
    "        for j in range(out_w):\n",
    "            conv[i][j] = np.sum(signal[i : kernel_h + i, j : kernel_w + j] * kernel)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\n",
    "# Signal\n",
    "# [[0,0,0,0],\n",
    "#  [1,2,2,1],\n",
    "#  [0,0,0,0]],\n",
    "signal: np.ndarray = np.array([\n",
    "    [0] * 4,\n",
    "    [1, 2, 2, 1],\n",
    "    [0] * 4,\n",
    "])\n",
    "# Kernel\n",
    "# [[-1,-1],\n",
    "#  [1,1]]\n",
    "kernel: np.ndarray = np.asarray([-1, -1, 1, 1]).reshape(2, 2)\n",
    "\n",
    "# Transpose by swapping strides\n",
    "transposed_signal = as_strided(\n",
    "    signal,\n",
    "    (signal.shape[1], signal.shape[0]),\n",
    "    strides=(signal.strides[1], signal.strides[0]),\n",
    ")\n",
    "logger.debug(transposed_signal)\n",
    "# [[0 1 0]\n",
    "#  [0 2 0]\n",
    "#  [0 2 0]\n",
    "#  [0 1 0]]\n",
    "\n",
    "# Flatten kernel\n",
    "flattened_kernel = kernel.reshape(-1)\n",
    "logger.debug(flattened_kernel)\n",
    "# [-1,-1,1,1]\n",
    "# We would like to create a view of the signal which is nx4 to allow for easy\n",
    "# broadcasting of the multiplication operation with the flattened_kernel.\n",
    "# We do this with striding.\n",
    "out_h, out_w = map(lambda t: t[0] - t[1] + 1, zip(signal.shape, kernel.shape))\n",
    "conv = conv2d_vectorized(signal, kernel, out_h, out_w)\n",
    "logger.info(\n",
    "    \"conv2d_vectorized: \"\n",
    "    + \"{:.5e}\".format(\n",
    "        timeit(\n",
    "            \"conv2d_vectorized( signal, kernel, *map(lambda t: t[0] - t[1] + 1, zip(signal.shape, kernel.shape)))\",\n",
    "            globals=globals(),\n",
    "            number=1000,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "conv_2 = conv2d_naive(signal, kernel, out_h, out_w)\n",
    "logger.info(\n",
    "    \"conv2d_naive: \"\n",
    "    + \"{:.5e}\".format(\n",
    "        timeit(\n",
    "            \"conv2d_naive( signal, kernel, out_h, out_w)\",\n",
    "            globals=globals(),\n",
    "            number=1000,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "assert np.array_equal(conv, conv_2)\n",
    "\n",
    "conv_3 = libsignal.correlate2d(signal, kernel, mode=\"valid\")\n",
    "logger.info(\n",
    "    \"signal.correlated2d: \"\n",
    "    + \"{:.5e}\".format(\n",
    "        timeit(\"libsignal.correlate2d( signal, kernel)\", globals=globals(), number=1000)\n",
    "    ),\n",
    ")\n",
    "assert np.array_equal(conv, conv_3)\n",
    "\n",
    "\n",
    "def conv2d_vectorized_batched_channelled(\n",
    "    signal: np.ndarray, kernel: np.ndarray, out_h: int, out_w: int\n",
    ") -> np.ndarray:\n",
    "    batch, channel = signal.shape[:2]\n",
    "    view_shape = (batch, channel, out_h, out_w, *kernel.shape[1:])  # (\n",
    "    view_strides = (\n",
    "        signal.strides[0],  # jump to next batch\n",
    "        signal.strides[1],  # jump to next channel\n",
    "        signal.strides[2],  # jump to next row in signal,\n",
    "        signal.strides[3],  # jump to next col in signal,\n",
    "        signal.strides[2],  # jump to next row in window,\n",
    "        signal.strides[3],  # jump to next col in window,\n",
    "    )\n",
    "    views = as_strided(signal, view_shape, view_strides)  # (2,2,2,3,2,2)\n",
    "    # [[0 0 1 2]\n",
    "    #  [0 0 2 2]\n",
    "    #  [0 0 2 1]\n",
    "    #  [1 2 0 0]\n",
    "    #  [2 2 0 0]\n",
    "    #  [2 1 0 0]]\n",
    "    return np.einsum(\"bchwkl,ckl->bchw\", views, kernel).reshape(\n",
    "        batch, channel, out_h, out_w\n",
    "    )\n",
    "\n",
    "\n",
    "batches = 1000\n",
    "channels = 3\n",
    "batched_channelled_signal = np.random.rand(batches, channels, *signal.shape)\n",
    "channelled_kernel = kernel.reshape(1, *kernel.shape)\n",
    "logger.debug(\n",
    "    result := conv2d_vectorized_batched_channelled(\n",
    "        batched_channelled_signal, channelled_kernel, out_h, out_w\n",
    "    )\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"conv2d_vectorized_batched_channelled: \"\n",
    "    + \"{:.5e}\".format(\n",
    "        tot := timeit(\n",
    "            \"libsignal.correlate2d( signal, kernel)\", globals=globals(), number=1000\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "logger.info(f\"Per batch/channel: {tot / (batches * channels):.5e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
