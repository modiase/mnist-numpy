# 2025-03-16

## Numerical stability issues with softmax

```python
def softmax(x: np.ndarray) -> np.ndarray:
    shifted_x = x - np.max(x, axis=1, keepdims=True)
    exp_x = np.exp(shifted_x)
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)
```

Softmax will potentially generate inf when the logits of the model become large. This is probably also a consequence of too large
a training step, but a mathematical trick ensures that the values of the logits will never exceed 0 and therefore exp(z) will never be greater than 1.

```latex
$$  \frac{exp(z_i - m)}{\sum_j{exp(z_j-m)}} $$

We can factor out the common $ exp(-m) $ in the numerator and denominator in order to recover the softmax function.

$$  \frac{exp(z_i)}{\sum_j{exp(z_j)}} $$
```

## Batching

A training epoch is a full batched cycle through the training set. A naive approach of batching the training set by taking a constant number of examples to produce fixed batches and then cycling through them may introduce generalization problems because the model is always trained on the same batches. It is better to randomize batches so that different examples are uniformly likely to appear in the same batch.


## Learning Rate

I have started observing numerical stability issues which depend on the choice of alpha. I belive I will need to investigate momentum based methods in order to understand how to use a variable learning rate to control the rate of descent and prevent oscillations.

I observed that applying the following diff solved the numerical stability issues at higher learning rates. I will need to investigate why this is the case.

```python
for idx in range(len(self._W) - 1, -1, -1):
    dW = k * (_A[idx].T @ dZ)
    db = k * np.sum(dZ, axis=0)
    if np.isnan(dW).any() or np.isnan(db).any() or np.isnan(dZ).any():
        raise ValueError("Invalid gradient. Aborting training.")    
-   self._W[idx] -= learning_rate * dW
-   self._b[idx] -= learning_rate * db
    if idx > 0:
        dZ = (dZ @ self._W[idx].T) * deriv_ReLU(Z[idx - 1])
+   self._W[idx] -= learning_rate * dW
+   self._b[idx] -= learning_rate * db
```